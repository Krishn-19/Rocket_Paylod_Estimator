# Rocket Paylod Predictor
Synopsis:
A small pipeline that generates physically-plausible synthetic rocket datasets, trains multi-output regressors to predict payload to different orbits (LEO, ISS, SSO, MEO, GEO), and validates those trained models against real rocket data while producing figures and CSV summaries.

## Table of contents
- [Project Overview](#project-overview)
- [What’s included / Repo layout](#whats-included--repo-layout)
- [Core ideas & approach](#core-ideas--approach)
- [Requirements Installation](#requirements)
- [How to run (examples)](#how-to-run-examples)
- [Script reference (what each file does)](#script-reference-what-each-file-does)
  - [synthetic_data_generation.py](#synthetic_datagenerationpy)   <!-- optional internal anchor format for file names -->
  - [feature_importance_analysis.py](#feature_importance_analysispy)
  - [training_and_validation.py](#training_and_validationpy)
  - [synthetic_vs_real_data_visualisation.py](#synthetic_vs_real_data_visualisationpy)
  - [real_data.py](#real_datapy)
- [Outputs & folders produced](#outputs--folders-produced)
- [Tips, common errors & troubleshooting](#tips-common-errors--troubleshooting)
- [Contribution](#contribution)
- [Changelog (v1)](#changelog-v1)

## Project Overview
This project is intended to (1) build a large synthetic rocket dataset from a small, curated set of real rockets, (2) train multi-output regression models to predict payload capacities for different target orbits, and (3) validate the trained models on a real rocket dataset. The focus is on transparency (intermediate reports/plots), reproducibility (random seeds), and diagnostics (feature importance, error analysis, per-rocket comparisons).

It is useful for:
1. Exploring how rocket design variables (stage masses, isp, ∆v, thrust) map to payload capacity.
2. Rapidly creating training data for ML experiments when real datasets are small.
3. Comparing modelling choices (RandomForest / LightGBM / XGBoost).

This is v1 of the project currently.

## What’s included / Repo layout
```
├── data/
│   ├── rockets_pivoted.xlsx              # (REQUIRED) real input pivoted excel
│   ├── synthetic_rockets_pivoted.xlsx    # (generated by script)
│   ├── synthetic_rockets_pivoted.csv     # (generated)
│   └── synthetic_generation_report.json  # (generated)
├── results/                              # created automatically (plots, tables, models, validation)
│   ├── plots/
│   ├── tables/
│   ├── models/
│   └── validation/
├── src/
│   ├── synthetic_data_generation.py
│   ├── feature_importance_analysis.py
│   ├── training_and_validation.py
│   ├── synthetic_vs_real_data_visualisation.py
│   └── real_data.py
├── requirements.txt  (suggested)
└── README.md
```
## Core ideas & approach
1. Synthetic generation: synthetic_data_generation.py reads a compact pivoted Excel (data/rockets_pivoted.xlsx) and builds N synthetic rockets using physical heuristics (rocket equation, structural fraction, liftoff thrust-to-weight) and empirical sampling. Output is pivoted to the same format as input so later scripts can treat synthetic and real data uniformly.

2. Feature engineering: The generator creates stage-level flattened keys (e.g., stage1_final_mass_kg) and global metrics (liftoff_TW, sum_delta_v_m_s).

3.  Feature importance: feature_importance_analysis.py trains the three model families on the synthetic data to output top features for payload prediction (helps design feature sets).

4. Training & validation: training_and_validation.py pre-processes, removes outliers (IQR), trains multi-output RandomForest / LightGBM / XGBoost models (separately for rockets with and without a transfer stage), saves model info, plots predictions vs actuals, and validates against rockets_pivoted.xlsx real rockets.

5. Visualization / diagnostics: synthetic_vs_real_data_visualisation.py and real_data.py provide quick checks to compare distributions and eyeball real vs. synthetic.

## Requirements Installation
### Clone repo:
```
git clone <your-repo-url>
cd <your-repo-name>
```
### Create & activate virtual environment:
macOS / Linux:
```
python3 -m venv venv
source venv/bin/activate
```
Windows (PowerShell):
```
python -m venv venv
.\venv\Scripts\Activate.ps1
```

### Install dependencies:
```
pip install -r requirements.txt
```
or install packages manually if you prefer.

## How to run (examples)
Make sure data/rockets_pivoted.xlsx exists before running the generator/training.

1. Generate synthetic dataset
```
python src/synthetic_data_generation.py
```
Output: data/synthetic_rockets_pivoted.xlsx, data/synthetic_rockets_pivoted.csv, data/synthetic_generation_report.json.

2. Quick look at the real data
```
python src/real_data.py
```
Prints head of real dataset and columns.

35. Inspect real vs synthetic visual comparison
```
python src/synthetic_vs_real_data_visualisation.py
```
Creates plots comparing selected key parameters side-by-side and a scatter matrix.

4. Feature importance analysis
```
python src/feature_importance_analysis.py
```
Trains RF/LGB/XGB on synthetic data and prints / plots top 10 features for rockets with and without transfer stage.

5. Train models and validate on real rockets
```
python src/training_and_validation.py
```
Full pipeline: preprocessing → outlier removal → train models for both rocket groups (with/without transfer) → save plots/tables/models → load real data and run validation → create plots & tables with metrics and error analyses.
The script creates results/ and subfolders automatically.


## Script reference (what each file does)
### src/synthetic_data_generation.py
Generate N_SYNTH synthetic rockets by sampling empirical distributions derived from data/rockets_pivoted.xlsx and by using physics-based heuristics (rocket equation, structural fraction, liftoff TW). Saves pivoted synthetic dataset and a small JSON report.

### src/feature_importance_analysis.py
Trains RandomForest/LightGBM/XGBoost multioutput regressors on the synthetic dataset to return top features for payload predictions for the two rocket categories (with/without transfer stage). Plots bar charts.

### src/training_and_validation.py
The main training & validation pipeline. Steps:

1. Load synthetic .csv, transpose to rockets-as-rows.
2. Split rockets into with_transfer and without_transfer sets.
3. Preprocess (flatten multiindex columns, fillna, convert numeric).
4. Remove outliers via IQR on target payloads.
5. Train MultiOutput models (Random Forest, LightGBM, XGBoost) with pre-defined feature sets (or all features).
6. Save plots (predicted vs actual), CSV tables (train/test results), and a simple model parameter text file in results/models/.
7. Load real rockets (rockets_pivoted.xlsx) and validate trained models; produce summary tables and validation plots.

### src/synthetic_vs_real_data_visualisation.py
Compare selected parameters between the real dataset and a sample of synthetic rockets and draw scatter-matrix for relationships.

### src/real_data.py
Minimal utility to inspect the real pivoted Excel input.


## Outputs & folders produced
When you run the training pipeline, the project creates a results/ folder with subfolders:

1. results/plots/ — PNG figures (training/test scatterplots, comparisons, error analysis).
2. results/tables/ — CSV exports of train/test metrics, validation tables.
3. results/models/ — .txt files with model parameter dumps (you can extend to save full pickled models).
4. results/validation/ — CSVs and PNGs related to validation on real rockets.

Filenames include timestamps (YYYYMMDD_HHMMSS) for reproducible tracking.

## Tips, common errors & troubleshooting
1. FileNotFoundError: rockets_pivoted.xlsx — ensure data/rockets_pivoted.xlsx exists. The generator expects the original pivoted format (Stage, Parameter rows; rockets as columns).

2. Excel reading issues — ensure openpyxl installed, or pass engine='openpyxl' to pd.read_excel.

3. LightGBM / XGBoost install problems — these sometimes require a C++ compiler or binary wheels. On Windows, use conda or prebuilt wheels. On Linux/macOS, ensure compilers are available or install from conda-forge.

4. Long run-times / memory — training on 18k samples with multi-output models is moderate but may take time. If you run out of memory, reduce N_SYNTH or use n_jobs parameters if models support them.

5. Missing features warnings — training and validation code warns when requested features are not present in X; use feature_importance_analysis.py to update the feature list.

## Contribution
This is v1 — feel free to open issues or pull requests.

Suggested workflow:
Fork the repo.
Create a feature branch: git checkout -b feat/awesome-improvement
Make changes, run scripts locally.
Submit PR with summary, tests (if any), and expected outputs.

Please follow the code style already present (simple procedural Python scripts, modular functions).